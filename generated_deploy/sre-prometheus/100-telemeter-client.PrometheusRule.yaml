apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: sre-telemeter-client
    role: alert-rules
  name: sre-telemeter-client
  namespace: openshift-monitoring
spec:
  groups:
  - name: sre-telemeter-client
    rules:
    - alert: MetricsClientSendFailingSRE
    # metricsclient sends 1 request/5m, 12/60m, 288/day, 8064/28days.
    # SLO=90%, Error Budget=10% or 806 failed requests.
    # Send an alert when 2% of Error budget (16.12 requests) is burned within a 2 hour window. Why 2 hours? Because the combination of request frequency and SLO @90% requires atleast 2 hours to breach a 2% burn. 
    # The short window of 10 minutes allows for quick recovery once the issue is resolved. 
      expr: sum by (job) (rate(metricsclient_request_send{status_code!="200"}[10m])) / sum by (job) (rate(metricsclient_request_send[10m])) > (16.12 - (1-0.90000)) and sum by (job) (rate(metricsclient_request_send{status_code!="200"}[2h])) / sum by (job) (rate(metricsclient_request_send[2h])) > (16.12 - (1-0.90000))
      for: 10m
      labels:
        severity: critical
        namespace: openshift-monitoring
      annotations:
        description: "Telemeter client is experiencing a high error rate over the past hour"
